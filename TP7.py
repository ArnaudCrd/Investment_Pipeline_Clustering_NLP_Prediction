"""
Fine-tuning d‚Äôun mod√®le de sentiment financier

Auteurs : Wayan Crain, Arnaud Ch√©ridi

Description :
Ce script fine-tune des mod√®les de classification (BERT) pour analyser le
sentiment d‚Äôactualit√©s financi√®res. Il utilise deux jeux de donn√©es publics,
entra√Æne plusieurs mod√®les, et s√©lectionne automatiquement le meilleur
selon le F1-score pond√©r√©.

Fonctionnalit√©s :
- Chargement et fusion de deux jeux de donn√©es de classification financi√®re
- Pr√©traitement avec tokenisation (transformers)
- Entra√Ænement et √©valuation de mod√®les de type BERT
- S√©lection et sauvegarde du meilleur mod√®le et tokenizer
- Export des m√©triques de performance
"""

from datasets import load_dataset, concatenate_datasets, DatasetDict
import torch
import json
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
import os
os.environ["WANDB_DISABLED"] = "true"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1. Chargement des jeux de donn√©es
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def load_and_prepare_datasets():
    """
    Charge et fusionne deux jeux de donn√©es d‚Äôactualit√©s financi√®res.

    Returns:
        DatasetDict: Dictionnaire contenant deux sous-ensembles :
                     - "train" : ensemble d'entra√Ænement fusionn√©
                     - "test"  : ensemble de test fusionn√©
    """
    ds1 = load_dataset("zeroshot/twitter-financial-news-sentiment")
    ds2 = load_dataset("nickmuchi/financial-classification")

    if "label" not in ds2["train"].features:
        ds2 = ds2.rename_column("labels", "label")

    train_data = concatenate_datasets([ds1["train"], ds2["train"]])
    test_data  = concatenate_datasets([ds1["validation"], ds2["test"]])
    return DatasetDict({"train": train_data, "test": test_data})

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2. Tokenisation
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def tokenize_dataset(tokenizer, dataset):
    """
    Applique la tokenisation aux textes des jeux de donn√©es.

    Args:
        tokenizer (AutoTokenizer): Tokenizer HuggingFace.
        dataset (DatasetDict): Dictionnaire contenant "train" et "test".

    Returns:
        tuple: (tokenized_train, tokenized_test) datasets format√©s pour PyTorch.
    """
    def preprocess(example):
        return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)
    tokenized_train = dataset["train"].map(preprocess, batched=True)
    tokenized_test  = dataset["test"].map(preprocess, batched=True)

    tokenized_train.set_format("torch", columns=["input_ids", "attention_mask", "label"])
    tokenized_test.set_format("torch", columns=["input_ids", "attention_mask", "label"])
    return tokenized_train, tokenized_test

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3. Entra√Ænement et √©valuation
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def train_and_evaluate(model_name: str,
                       dataset: DatasetDict,
                       batch_size: int = 16,
                       num_epochs: int = 3):
    """
    Entra√Æne et √©value un mod√®le sur le jeu de donn√©es fourni.

    Args:
        model_name (str): Nom du mod√®le pr√©-entra√Æn√© (ex: 'bert-base-uncased').
        dataset (DatasetDict): Jeu de donn√©es complet ("train" + "test").
        batch_size (int): Taille de batch pour l'entra√Ænement.
        num_epochs (int): Nombre d‚Äô√©poques.

    Returns:
        tuple: (mod√®le entra√Æn√©, tokenizer, m√©triques d‚Äô√©valuation)
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model     = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

    X_train, X_test = tokenize_dataset(tokenizer, dataset)

    training_args = TrainingArguments(
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=num_epochs,
        weight_decay=0.01,
        do_eval=True,
    )

    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        preds = np.argmax(logits, axis=-1)
        acc = accuracy_score(labels, preds)
        prf = precision_recall_fscore_support(labels, preds, average="weighted")
        return {"accuracy": acc, "precision": prf[0], "recall": prf[1], "f1": prf[2]}

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=X_train,
        eval_dataset=X_test,
        compute_metrics=compute_metrics
    )

    trainer.train()
    metrics = trainer.evaluate()
    print(f"\nüìä R√©sultats pour {model_name}:")
    for k, v in metrics.items():
        print(f"{k}: {v:.4f}")
    return model, tokenizer, metrics

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4. Pipeline principal
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def pipeline():
    """
    Ex√©cute tout le pipeline de fine-tuning :
    - Chargement des donn√©es
    - Entra√Ænement de plusieurs mod√®les
    - S√©lection du meilleur (via F1-score)
    - Sauvegarde mod√®le, tokenizer et m√©triques

    Returns:
        None
    """
    dataset = load_and_prepare_datasets()

    candidates = [
        "bert-base-uncased",
        "yiyanghkust/finbert-tone"
    ]

    best_model = None
    best_tokenizer = None
    best_metrics = None
    best_f1 = -1

    for model_name in candidates:
        model, tokenizer, metrics = train_and_evaluate(model_name, dataset)
        if metrics["eval_f1"] > best_f1:
            best_f1 = metrics["eval_f1"]
            best_model = model
            best_tokenizer = tokenizer
            best_metrics = metrics

    # Sauvegarde du meilleur mod√®le
    output_dir = "models/LLM"
    os.makedirs(output_dir, exist_ok=True)
    best_model.save_pretrained(output_dir)
    best_tokenizer.save_pretrained(output_dir)
    with open(os.path.join(output_dir, "metrics.json"), "w") as f:
        json.dump(best_metrics, f, indent=4)
    print(f"\nMeilleur mod√®le sauvegard√© dans {output_dir} (F1 = {best_f1:.4f})")
